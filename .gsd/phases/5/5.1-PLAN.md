---
phase: 5
plan: 1
wave: 1
---

# Plan 5.1: App Telemetry Dashboard

## Objective
The Flutter App acts as the visual reference monitor for the Neural-Intent Engine. It simply listens to Port 82 on the Hub and displays the recognized gesture alongside the expanded AI speech.

## Context
- .gsd/SPEC.md
- hub/main_server.py (From Phase 4)

## Tasks

<task type="auto">
  <name>Build App Telemetry Service</name>
  <files>flutter_app/lib/services/telemetry_service.dart</files>
  <action>
    - If `flutter_app` doesn't exist, ignore this step (assuming it will be created dynamically or exists elsewhere in the repo). 
    - Otherwise, create `telemetry_service.dart`.
    - Use `web_socket_channel` to connect to `ws://[PC_HUB_IP]:82`.
    - Provide a Stream that decodes the JSON payload `{"type": "telemetry", "intent": "question", "speech": "Excuse me..."}`.
  </action>
  <verify>flutter analyze flutter_app/lib/services/telemetry_service.dart (if flutter exists)</verify>
  <done>Dart class successfully wrapped the WebSocket stream for UI consumption.</done>
</task>

<task type="auto">
  <name>Build Telemetry Screen</name>
  <files>flutter_app/lib/screens/dashboard_screen.dart</files>
  <action>
    - Create a sleek Dark Mode UI.
    - Top half: Large text showing the raw detected Sign (e.g. `[ SIGN DETECTED: "QUESTION" ]`).
    - Bottom half: The final generated spoken sentence (e.g. `Translation: "Excuse me, Professor..."`).
    - Use a `StreamBuilder` connected to the `TelemetryService` to update these values instantly.
  </action>
  <verify>flutter analyze flutter_app/lib/screens/dashboard_screen.dart (if flutter exists)</verify>
  <done>UI component is capable of reactive updates based on the WebSocket.</done>
</task>

## Success Criteria
- [ ] The App successfully visualizes the otherwise invisible data flow of the PC Hub.
