# Architecture — SYNAPSE

> Evolved from HealthGuard. Auto-generated by /map, refined during deep project rethink.

## Overview

SYNAPSE (formerly HealthGuard) is a **full-duplex communication suit** for deaf-mute students. It combines a Flutter mobile app with IoT hardware (smart glove) and on-device AI to translate gestures into fluent, emotion-modulated speech while transcribing classroom audio locally.

The existing HealthGuard vitals infrastructure (Health Connect, backend sync, AI chat/coaching, spike alerts) is **fully preserved** and serves as the health-monitoring backbone.

## System Layers

```
┌─────────────────────────────────────────────────────┐
│               HARDWARE LAYER (IoT)                   │
│  Arduino Uno ← Flex×5 + MPU6050 + Pulse Sensor      │
│       │ Serial / USB                                 │
│  Bone Conduction Transducer ← PAM8403 Amp ← Arduino  │
└───────┼──────────────────────────────────────────────┘
        │ Serial / Local Connection
┌───────┼──────────────────────────────────────────────┐
│       │          PC HUB LAYER                        │
│       │ (The Brain & Voice Translation Hub)          │
│                                                      │
│  Whisper STT (PC / Python / Local)                   │
│  GestureEngine (7D cosine similarity on PC)          │
│  IntentService (Gemini context gen on PC)            │
│  VoiceService (Audio gen + Arduino output)           │
│  WebSocket Server (Receives Vitals from Phone)       │
└───────┼──────────────────────────────────────────────┘
        │ WiFi (WebSocket / REST)
┌───────┼──────────────────────────────────────────────┐
│       │        FLUTTER APP LAYER                     │
│       │ (Vitals & Dashboard Client)                  │
│                                                      │
│  HealthService      — Health Connect vitals          │
│  TelemetryClient    — Steams Vitals to PC Hub        │
│  ApiService         — Backend REST client            │
│  ChatService        — Gemini AI chat                 │
│  Dashboard          — View status & remote data      │
└──────────────────────────────────────────────────────┘
        │
┌───────┼──────────────────────────────────────────────┐
│       │        BACKEND LAYER (Existing)              │
│  Node.js + Express + SQLite                          │
│  Tables: Doctor, Patient, Vitals, Appointments       │
│  Routes: /patient, /doctor                           │
└──────────────────────────────────────────────────────┘
```

## Shift in Architecture (PC as Hub)

The role of the Flutter app has changed significantly from the original plans. 

1. **PC Hub**: The PC is now the central compute hub for the physical hardware. It reads the Arduino (via serial line), runs the Whisper STT, does the Gesture matching, pings Gemini for Intent generation, and outputs TTS to the bone conduction transducer via the connected Arduino/amp.
2. **Flutter App**: The Flutter app is now primarily a health-data streaming source. It reads Apple Health/Health Connect vitals and streams them to the PC Hub over WiFi so the VoiceService can modulate the TTS emotion based on the student's live heart rate. It also retains its dashboard and AI chat functionalities.

## Existing Components (HealthGuard — Preserved)

### Features
Auth, Dashboard, Chat Assistant, Coaching, Vitals, Reminders, Profile, Book Appointment

### Models
UserVitals, VitalReading, BloodPressureReading, ChatMessage, CoachingPlan, RiskResult

### Services
HealthService, ApiService, ChatService, CoachingService, SpikeAlertService

### Backend
Express + SQLite with Doctor/Patient/Vitals/Appointments tables

## New Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| whisper_ggml_plus | ^1.3.3 | On-device Whisper inference |
| web_socket_channel | ^3.0.3 | WebSocket connection to ESP8266 |
| record | ^5.1.2 | Audio recording for Whisper input |
| path_provider | ^2.1.2 | Temp file storage for audio and model |

## Technical Debt (Carried Forward)

- [ ] Gemini API key hardcoded in `config/ai_config.dart`
- [ ] Passwords stored in plaintext (no hashing)
- [ ] No JWT/session management
- [x] ~~`google_generative_ai` in dev_dependencies~~ → Fixed: moved to production dependencies
- [x] ~~`speech_to_text` unused dependency~~ → Fixed: removed (Whisper replaces it)
- [ ] No automated tests
